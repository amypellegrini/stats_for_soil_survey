---
title: "7_Generalized_linear_models"
author: "Stephen Roecker"
date: "January 6, 2016"
output: 
 html_document: 
    toc: yes
---


# Introduction

Generalized linear models (GLM) as the name implies are a generalization of the linear modeling framework to allow for the modeling of response variables (e.g. soil attributes) with non-normal distributions and heterogeneous variances. Whereas linear models are designed for predicting continuous soil properties such as clay content or soil temperature, GLM can be used to predict the presence/absence of argillic horizons (i.e. logistic regression) or counts of a plant species along a transact (i.e. Poisson regression). These generalizations greatly expands the applicability of the linear modeling framework, while still allowing for a similar fitting procedure and interpretation of the resulting models.

In the past in order to handle non-linearity and heterogeneous variances, transformations have been made to the response variable, such as the log(x). However such transformations complicate the models interpretation because the results refer to the transformed scale (e.g. log(x)). Also previous transformations were not guaranteed to achieve both normality and constant variance simultaneously. GLM likewise transform the response, but also preserve the scale of the response and provide separate functions to transform the mean response and variance, know as the link and variance functions respectively. So instead of looking like this

$f(y) = \beta_{0} + \beta_{1}x + \varepsilon$

you get this

$g(\mu)$ or $\eta = \beta_{0} + \beta_{1}x + \varepsilon$

with $g(\mu)$ or $\eta$ symbolizing the link function. 

Another alteration the classical linear model is that with GLM the coefficients are estimated iteratively by maximum likelihood estimation instead of ordinary least squares. This results in the GLM minimizing the deviance, instead of the sum of squares. However for the Gaussian (i.e. normal) distributions the deviance and sum of squares are equivalent.


# Logistic regression

Logistic regression is a specific type of GLM designed to model data that has a binomial distribution (i.e. presence/absence, yes/no, or proportional data), which in statistical learning parlance is considered a classification problem. For binomial data the logit link transform is used. The effect of the logit transform can be seen in the following figure. It creates a sigmoidal curve, which enhances the separation between the two groups. It also has the effect of ensuring that the values range between 0 and 1.


```{r, echo=FALSE}
p <- c(rbinom(100, 1:100, 1)) / 100
p <- sort(p)
logp <- log(p / (1 - p))
test <- data.frame(p = p, logp = logp)[-length(p), ]
fit <- lm(p ~ logp, data = test)

plot(logp, p, type = "l", ylab = "proportion (p)", xlab = "logit transform (log(p / (1 - p)))", ylim = c(0, 1))
```

When comparing a simple linear model vs a simple logistic model we can see the effect of the logit transform on the relationship between the response and predictor variable. As before it follows a sigmoidal curve and prevents predictions from exceeding 0 and 1. 

```{r, echo=FALSE}
load(file = "C:/workspace/stats_for_soil_survey/trunk/data/ch7_data.Rdata")
test <- na.exclude(data)
test <- subset(test, select = c(argillic.horizon, twi))
names(test) <- c("response", "predictor")
test$predictor <- abs(test$predictor - 13.7)
test <- test[order(test$predictor), ] 

lm_fit <- lm(response ~ predictor, data = test)

glm_fit <- glm(response ~ predictor, data = test, family = binomial())

par(mfrow = c(1, 3))
    plot(test$predictor, lm_fit$fitted.values, type = "l", ylim = c(0, 1), ylab = "linear fit", xlab = "predictor")
    points(test$predictor, test$response)
    plot(test$predictor, glm_fit$fitted.values, type = "l", ylim = c(0, 1), ylab = "logistic fit", xlab = "predictor")
    points(test$predictor, test$response)
    boxplot(predictor ~ response, data = test, ylab = "predictor", xlab = "response", col = "grey")
```

# Logistic regression example

Now that we've got some of the basic GLM theory out of the way we'll move on to a real example, and address any additional theory where it relates to specific steps in the modeling process. The examples selected for this chapter comes from the Mojave desert (where I've dug a few holes). The problem tackled here is a familiar one, where can I expect to find a specific soil series on the landscape. In particular we're going to try and map the Jumborox and Helendale soil series. Both are typic haplargids and can be found on fan remnants, which are a stable landform that is a remnant of the Pleistocene. Despite the low relief of most fans, fan remnants are uplands in the sense that they generally don't receive run-on or deposition.

# Load packages

To start, as always we need to load some extra packages. This is a necessary evil everytime you start R. Most of the basic functions we need to develop a logistic regression model are contained in base R, but the following contain some useful spatial and data manipulation functions. Believe it or not we will use all of them and more.

```{r load packages, echo=FALSE, message=FALSE, warning=FALSE}
library(aqp) # specialized soil classes and functions
library(soilDB) # NASIS and SDA import functions
library(raster) # guess
library(rgdal) # spatial import
library(lattice) # graphing
library(reshape2) # data manipulation
library(plyr) # data manipulation
library(caret) # printing
library(rms) # additional regression modeling tools
```

# Read in data

Hopefully like all good soil scientists and ecological site specialists you enter your field data into NASIS. Better yet hopefully someone else did it for you.  Once data is captured in NASIS it much easier to import the data into R, extract the pieces you need, manipulate it, model it, etc. If it's not entered into NASIS it may as well not exist, and will it haunt you for the rest of your life.

```{r import data}
pedons <- fetchNASIS() # beware the error messages, uh oh looks like we have some, look the other way (by default they don't get imported)
#load(file = "C:/workspace/stats_for_soil_survey/trunk/data/ch7_data.Rdata")

str(pedons) # Examine the makeup of the data we imported from NASIS.
```

# Exploratory analysis

Generally before we begin modeling its good to explore the data. To start lets take a look at the number of soil series observed in our dataset.

```{r}
s <- site(pedons) # extract the site table
table(s$taxonname) # tabulate the number of soil series observed
```

I count almost 200 series. Luckily we have a decent sample of Jumborox and Helendale. However you'll notice some are misspelled and other curiously labeled. Assuming the misspelled series belong in sample and in order to help boost our sample size, we'll temporarily adjust these series names. At a minimum for modeling purposes we probably need 10 pedons for the series we're interested in and a total of 100 pedons overall.

```{r}
s$taxonname2 <- ifelse(grepl("Jumbo", s$taxonname), "Jumborox", s$taxonname) # search for the word "Jumbo" and if found replace with "Jumborox", otherwise keep the original taxonname
s$taxonname2 <- ifelse(grepl("Helen", s$taxonname), "Helendale", s$taxonname)
s$taxonname2 <- ifelse(s$taxonname2 == "Jumborox" | s$taxonname2 == "Helendale", s$taxonname2, "other")

table(s$taxonname2)
```

Another obvious place to look is at the geomorphic data in the site table. This information is intended to help differentiate where our soil observations exist on the landscape. If populated consistently if could be used in future dissaggregation efforts, as demonstrated by Nauman and Thompson, 2014.

```{r}
s$surface_gravel <- s$surface_gravel - s$surface_fgravel # recalculate gravel
s$surface_total <- apply(s[grepl("surface", names(s))], 1, sum) # calculate the total rock fragments

s_sub <- subset(s, select = c(taxonname2, slope_field, elev_field, surface_total)) # subset quantitative columns and taxonname
s_m <- melt(s_sub, id = "taxonname2") # convert s_sub to wide data format
head(s_m)
bwplot(taxonname2 ~ value | variable, data = s_m, scales = list(x = "free"))

table(s$landform.string, s$taxonname2) # cross tabulate landform vs taxonname

round(prop.table(table(s$hillslope_pos, s$taxonname2), 2) * 100) # cross tabulate and calculate proportions, the "2" calculates the proportions relative to the column totals
round(prop.table(table(paste(s$shapedown, s$shapeacross), s$taxonname2), 2) * 100)


# Argillic horizon by soil scientist, bias?
desc_test <- function(old) {
  old <- as.character(old)
  new <- NA
  # ranked by seniority
  if (is.na(old)) {new <- "other"}
  if (grepl("Judith", old)) {new <- "Judith"}
  if (grepl("Stephen", old)) {new <- "Stephen"} 
  if (grepl("Paul", old)) {new <- "Paul"} 
  if (grepl("Peter", old)) {new <- "Peter"}
  if (is.na(new)) {new <- "other"}
 return(new)
}

s$describer2 <- sapply(s$describer, desc_test)

table(s$describer2, s$taxonname2)
round(prop.table(table(s$describer2, s$taxonname2), 2) * 100)


# Plot coordinates
slot(pedons, "site") <- s # this is dangerous, but something needs to be fixed in the site() setter function
idx <- complete.cases(site(pedons)[c("x", "y")]) # create an index to filter out pedons that are missing coordinates in WGS84
pedons2 <- pedons[idx]
coordinates(pedons2) <- ~ x + y # add coordinates to the pedon object
proj4string(pedons2) <- CRS("+init=epsg:4326") # add projection to pedon object
 
ssa <- readOGR(dsn = "M:/geodata/soils", layer = "soilsa_a_nrcs") # read in soil survey area boundaries
ca794 <- subset(ssa, areasymbol == "CA794" | areasymbol == "CA698") # subset out Joshua Tree National Park
plot(ca794)
pedons_sp <- as(pedons2, "SpatialPointsDataFrame")
plot(pedons_sp, add = TRUE)
# Beware some points that fall outside of CA794 are not show here. Some are way outside of CA794.

pedons_sp <- spTransform(pedons_sp, CRS("+init=epsg:5070"))
# writeOGR(pedons_sp, dsn = "M:/geodata/project_data/8VIC", "pedon_locations2", driver = "ESRI Shapefile") # write shapefile of pedons

```

# Extract geodata at pedons locations

Prior to any spatial analysis or modeling, you need to develop a suite of geodata files that can be intersected with your field data locations. This is in and of itself is a difficult task, and should be facilitated by your Regional GIS Specialist. Typically this would primarily consist of derivatives from a DEM or satellite imagery. Prior to any prediction it is also necessary to ensure the geodata files have the same projection, extent, and cell size. Once we have the necessary files we can construct a list in R of the file names and paths, read the geodata into R and extract the geodata values where they intersect with your field data locations.

```{r geodata extract}
folder <- "M:/geodata/project_data/8VIC/"
files <- list(
  elev   = "ned30m_8VIC.tif",
  slope  = "ned30m_8VIC_slope5.tif",
  aspect = "ned30m_8VIC_aspect5.tif",
  twi    = "ned30m_8VIC_wetness.tif",
  z2str  = "ned30m_8VIC_z2stream.tif",
  mrrtf  = "ned30m_8VIC_mrrtf.tif",
  mrvbf  = "ned30m_8VIC_mrvbf.tif",
  solar  = "ned30m_8VIC_solar.tif",
  precip = "prism30m_8VIC_ppt_1981_2010_annual_mm.tif",
  precipsum = "prism30m_8VIC_ppt_1981_2010_summer_mm.tif",
  temp   = "prism30m_8VIC_tmean_1981_2010_annual_C.tif",
  ls     = "landsat30m_8VIC_b123457.tif",
  tc     = "landsat30m_8VIC_tc123.tif",
  k      = "gamma30m_8VIC_namrad_k.tif",
  th     = "gamma30m_8VIC_namrad_th.tif",
  u      = "gamma30m_8VIC_namrad_u.tif"
  )
geodata <- lapply(files, function(x) paste0(folder, x)) 

geodata_df <- data.frame(
  as.data.frame(pedons_sp)[c("taxonname2", "argillic.horizon", "x_std", "y_std", "describer2")],
  extract(stack(geodata), pedons_sp)
  )
names(geodata_df)[names(geodata_df) == "ls_6"] <- "ls_7"

data <- subset(geodata_df, select = - c(x_std, y_std))

# site(pedons)[c("describer", "x", "y", "x_std", "y_std", "utmnorthing", "utmeasting", "classifier")] <- NA
# data[c("describer", "x_std", "y_std")] <- NA
# save(data, ca794, pedons, file = "C:/workspace/stats_for_soil_survey.git/trunk/data/ch7_data.Rdata")
```

```{r More }
#load(file = "C:/workspace/stats_for_soil_survey.git/trunk/data/ch7_data.Rdata")
data <- data[complete.cases(data[c(1, 4:26)]), ]
data_m <- subset(data, select = - c(describer2))
data_m <- melt(data_m, id = "taxonname2")
bwplot(taxonname2 ~ value | variable, data = data_m, scales = list(x = "free"))

# Argillic horizons seem to occur over a limited range of twi and z2str. So lets rescale those variables by substracting their median
data$argillic.horizon <- data$mrvbf > 0.15 & data$argillic.horizon == TRUE # Subset out argillic horizons that only occur on fans. Argillic horizons that occur on hills and mountains more than likely form by different process, and therefore would require a different model.
aggregate(data[c("twi", "z2str", "tc_3")], list(data$argillic.horizon), median)
data$twi_sc <- abs(data$twi - 13.9)
data$z2str_sc <- abs(data$z2str - 12.9)
data$tc_3_sc <- abs(data$tc_3 - 28)

data$jumborox <- data$taxonname2 == "Jumborox"
data$helendale <- data$taxonname2 == "Helendale"

data2 <- subset(data, select = - c(describer2, argillic.horizon, helendale, taxonname2))

test <- glm(jumborox ~ ., data = data3, family = binomial(link = "cloglog"))
summary(test)
confusionMatrix(test$fitted.values > 0.5, as.logical(test$y), positive = "TRUE")

test2 <- glm(jumborox ~ 1, data = data2, family = binomial(link = "cloglog"))
summary(test2)
confusionMatrix(test2$fitted.values > 0.35, as.logical(test2$y), positive = "TRUE")
```



