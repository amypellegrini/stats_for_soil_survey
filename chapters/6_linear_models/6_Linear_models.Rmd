---
title: Chapter 6 - Linear Regression
author: Stephen Roecker, Katey Yoast
date: "Thursday, January 14, 2016"
output:
  html_document:
    keep_md: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: false
---


![Statistics for pedologists course banner image](figure/logo.jpg)

## Introduction
Linear regression has been used for soil survey applications since the early 1900s when Briggs and McLane (1907) developed a pedotransfer function to estimate the wilting coefficient as a function of soil particle size: Wilting coefficient = 0.01(sand) + 0.12(silt) + 0.57(clay). Linear regression, as the name implys, models the linear relationship between a dependent variable (y) and an independent variable (x) where y = b0 + b1x + e (b0=intercept of the fitted line, b1=slope of the fitted line, and e= the error term). When more than one independent variable is used in the regression, the model is called multiple linear regression. In regression models, the dependent (or response) variable must always be continuous. The independent (or predictor) variable(s) can be continuous, discrete, or categorical. In order to use linear regression or any linear model, the data must be normally distributed. Most environmental data are non-linear and require transformations to the response variable (such as square root or log) for use in linear models. Often, these data are best used in non-linear models due to the complications of interpreting and predicting transformed data. Normality can be assessed in R visually using a QQ plot or histogram and tabularly using something like the Shapiro-Wilk test of normality. 

```{r load packages, message=FALSE, warning=FALSE}
library(aqp) # specialized soil classes and functions
library(soilDB) # NASIS and SDA import functions
library(raster) # guess
library(rgdal) # spatial import
library(lattice) # graphing
library(reshape2) # data manipulation
library(plyr) # data manipulation
library(caret) # printing
library(car) # additional regression tools
```


### Read in data

Hopefully like all good soil scientists and ecological site specialists you enter your field data into NASIS. Better yet hopefully someone else did it for you. Once data is captured in NASIS it much easier to import the data into R, extract the pieces you need, manipulate it, model it, etc. If it's not entered into NASIS it may as well not exist.

```{r import data}
# pedons <- fetchNASIS(rmHzErrors = FALSE) # beware the error messages, by default they don't get imported unless you override the default, which in our chase shouldn't cause any problems
load(file = "C:/workspace/ch7_data.Rdata")

str(pedons, max.level = 2) # Examine the makeup of the data we imported from NASIS.
```


### <a id="wrang")></a> 7.4.1 Data wrangling

Generally before we begin modeling its good to explore the data. By using a simple summary we can quickly see the breakdown of how many argillic horizons we have. Unfortunately, odds are all the argillic horizons haven't been properly populated in the diagnostic horizon table like they should be. Luckily for us, the desert argillic horizons always pop up in the taxonomic name, so we can use pattern matching to extract it. By doing this we gain an additional 11 pedons with argillic horizons and are able to label the missing values (i.e. NA). At a minimum for modeling purposes we probably need 10 pedons of the target we're interested in and a total of 100 observations overall.


```{r consistency}
s <- site(pedons)

s$surface_gravel <- with(s, surface_gravel - surface_fgravel) # recalculate gravel
s$frags <- apply(s[grepl("surface", names(s))], 1, sum)

summary(s$frags)

densityplot(~ frags + surface_gravel, data = s, auto.key = TRUE)
```



### <a id="geomorph")></a> 7.4.2 Geomorphic data

Another obvious place to look is at the geomorphic data in the site table. This information is intended to help differentiate where our soil observations exist on the landscape. If populated consistently it could be used in future disaggregation efforts, as demonstrated by Nauman and Thompson (2014).


```{r landform}
# Landform vs frags

test <- aggregate(frags ~ landform.string, data = s, function(x) round(quantile(x, probs = c(0.05, 0.5, 0.95))))
arrange(test, frags[, 2]) # sort data frame by column using plyr package function

# or

# test[order(test$surface_total[, 2]), ]

s$landform <- ifelse(grepl("fan", s$landform.string), "fan", "hill")
s$landform <- as.factor(s$landform)

densityplot(~ frags + surface_gravel | landform, data = s, auto.key = TRUE)


# Hillslope position

test <- aggregate(frags ~ landform + hillslope_pos, data = s, function(x) round(quantile(x, probs = c(0.05, 0.5, 0.95))))
arrange(test, landform, frags[, 2])

# Slope shape

test <- aggregate(frags ~ landform + paste(shapedown, shapeacross), data = s, function(x) round(quantile(x, probs = c(0.05, 0.5, 0.95))))
arrange(test, landform, frags[, 2])


# Surface morphometry, depth and surface rock fragments

s_fan <- subset(s, landform == "fan", select = c(frags, surface_gravel, bedrckdepth, slope_field, elev_field))
s_hill <- subset(s, landform == "hill", select = c(frags, surface_gravel, bedrckdepth, slope_field, elev_field))

round(cor(s_fan, use = "pairwise.complete.obs"), 2)
round(cor(s_hill, use = "pairwise.complete.obs"), 2)

spm(s_fan, use = "pairwise.complete.obs")
spm(s_hill, use = "pairwise.complete.obs")

# Function to filter out the top 3 mappers
desc_test <- function(old) {
  old <- as.character(old)
  new <- NA
  # ranked by seniority
  if (is.na(old)) {new <- "other"}
  if (grepl("Stephen", old)) {new <- "Stephen"} 
  if (grepl("Paul", old)) {new <- "Paul"} 
  if (grepl("Peter", old)) {new <- "Peter"}
  if (is.na(new)) {new <- "other"}
 return(new)
}

s$describer2 <- sapply(s$describer, desc_test)

test <- aggregate(frags ~ landform + describer2, data = s, function(x) round(quantile(x, probs = c(0, 0.5, 1))))
test[order(test$landform, test$frags[, 2]), ]
```


### <a id="plot")></a> 7.4.4 Plot coordinates

Where do are our points plot. We can plot the general location in R, but for this task will export them to a Shapefile, so we can view them in a proper GIS, and look more closely. Notice in the figure below the number of points that fall outside the survey boundary. What it doesn't show is the points in the Ocean or Mexico.

```{r plot}
# Plot coordinates
pedons2 <- pedons

slot(pedons2, "site") <- s # this is dangerous, but something needs to be fixed in the site() setter function
idx <- complete.cases(site(pedons2)[c("x", "y")]) # create an index to filter out pedons with missing coordinates
pedons2 <- pedons2[idx]
coordinates(pedons2) <- ~ x + y # set the coordinates
proj4string(pedons2) <- CRS("+init=epsg:4326") # set the projection

ssa <- readOGR(dsn = "M:/geodata/soils/soilsa_a_nrcs.shp", layer = "soilsa_a_nrcs") # read in soil survey area boundaries
ca794 <- subset(ssa, areasymbol == "CA794") # subset out Joshua Tree National Park
# ca794 <- spTransform(ca794, CRS("+init=epsg:5070"))
pedons_sp <- as(pedons2, "SpatialPointsDataFrame") # coerce to sp object

plot(ca794, axes = TRUE)
plot(pedons_sp, add = TRUE) # notice the points outside the boundary

pedons_sp <- spTransform(pedons_sp, CRS("+init=epsg:5070")) # transform between coordinate systems
# optionally write shapefile of pedons
# writeOGR(pedons_sp, dsn = "F:/geodata/project_data/8VIC", "pedon_locations", driver = "ESRI Shapefile") 
```



#### <a id="view")></a> Exercise 1: View the data in ArcGIS

- Examine the shapefile in ArcGIS along with our potential predictive variables (hint classify the Shapefile symbology using the argillic horizon column)
- Discuss with your group, and report your observations or hypotheses


### <a id="extract")></a> 7.4.5 Extracting spatial data

Prior to any spatial analysis or modeling, you need to develop a suite of geodata files that can be intersected with your field data locations. This is in and of itself is a difficult task, and should be facilitated by your Regional GIS Specialist. Typically this would primarily consist of derivatives from a DEM or satellite imagery. Prior to any prediction it is also necessary to ensure the geodata files have the same projection, extent, and cell size. Once we have the necessary files we can construct a list in R of the file names and paths, read the geodata into R and extract the geodata values where they intersect with field data.

```{r extract, eval=FALSE}
folder <- "M:/geodata/project_data/8VIC/ca794/"
files <- c(
  elev   = "ned30m_8VIC.tif",
  slope  = "ned30m_8VIC_slope5.tif",
  aspect = "ned30m_8VIC_aspect5.tif",
  twi    = "ned30m_8VIC_wetness.tif",
#  twi_sc = "ned30m_8VIC_wetness_sc.tif",
  ch     = "ned30m_8VIC_cheight.tif",
  z2str  = "ned30m_8VIC_z2stream.tif",
  mrrtf  = "ned30m_8VIC_mrrtf.tif",
  mrvbf  = "ned30m_8VIC_mrvbf.tif",
  solar  = "ned30m_8VIC_solar.tif",
  precip = "prism30m_8VIC_ppt_1981_2010_annual_mm.tif",
  precipsum = "prism30m_8VIC_ppt_1981_2010_summer_mm.tif",
  temp   = "prism30m_8VIC_tmean_1981_2010_annual_C.tif",
  ls     = "landsat30m_8VIC_b123457.tif",
  tc     = "landsat30m_8VIC_tc123.tif",
  k      = "gamma30m_8VIC_namrad_k.tif",
  th     = "gamma30m_8VIC_namrad_th.tif",
  u      = "gamma30m_8VIC_namrad_u.tif",
  cluster = "cluster152.tif"
  )

geodata_f <- sapply(files, function(x) paste0(folder, x)) # combine the folder directory and file names

head(geodata_f)

geodata_r <- stack(geodata_f) # create a raster stack

data <- data.frame(
   as.data.frame(pedons_sp)[c("pedon_id", "taxonname", "frags", "x_std", "y_std", "describer2", "landform.string", "landform", "argillic.horizon")],
   extract(geodata_r, pedons_sp)
   )

datacluster <- as.factor(data$cluster)


# s[c("describer", "describer2", "x", "y", "x_std", "y_std", "utmnorthing", "utmeasting", "classifier")] <- NA
# slot(pedons, "site") <- s
# data[c("describer2", "x_std", "y_std")] <- NA
# save(data, ca794, pedons, file = "C:/workspace/ch7_data.Rdata")
```


### <a id="spatial")></a> 7.4.6 Examine spatial data 

With our spatial data in hand, we can now see whether any of the varibles will help us separate the presense/absense of argillic horizons. Because we're dealing with a classification problem, we'll compare the numeric variables using boxplots. What we're looking for are variables with the least amount of overlap in their distribution (i.e. the greatest separation in their median values).  

```{r spatial}
load(file = "C:/workspace/ch7_data.Rdata")
train <- data

train <- subset(train, mrvbf > 0.5 & landform != "hill" & frags < 100, select = - c(pedon_id, taxonname, frags, landform.string, x_std, y_std, landform)) # Include only argillic horizons that only occur on fans. Argillic horizons that occur on hills and mountains more than likely form by different process, and therefore would require a different model.

terrain1 <- c("slope", "solar", "mrrtf", "mrvbf")
terrain2 <- c("twi", "z2str")
climate <- c("elev", "precip", "precipsum", "temp")
landsat <- paste0("ls_", 1:6)
tcaps <- paste0("tc_", 1:3)
grad <- c("k", "th", "u")

round(cor(train[c("frags", terrain1)]), 2)
round(cor(train[c("frags", terrain2)], use = "pairwise.complete.obs"), 2)
round(cor(train[c("frags", climate)]), 2)
round(cor(train[c("frags", landsat)]), 2)
round(cor(train[c("frags", tcaps)], use = "pairwise.complete.obs"), 2)
round(cor(train[c("frags", grad)]), 2)

scatterplotMatrix(train[c("frags", terrain1)])
scatterplotMatrix(train[c("frags", terrain2)])
scatterplotMatrix(train[c("frags", climate)])
scatterplotMatrix(train[c("frags", tcaps)])
scatterplotMatrix(train[c("frags", grad)])

bwplot(frags ~ as.factor(cluster), data = train)

bwplot(twi ~ landform, data = train)
aggregate(twi ~ landform, data = train, median)
train$twi_sc <- abs(train$twi - 13.8)
train$gsi <- with(train, (ls_3 - ls_1) / (ls_3 + ls_2 + ls_1))
train$sw <- cos(train$aspect - 255)

```



```{r}
full <- lm(frags ~ . + tc_3 * mrvbf, data = train) # "~ ." includes all columns in the data set
null <- lm(frags ~ 1, data = train) # "~ 1" just includes an intercept

add1(null, full, test = "F") # using the AIC test the effect of '

null <- update(null, . ~ . + tc_3) # add twi_sc to the model, 

test2 <- lm(frags ~ elev + gsi, data = test)


test_sub <- subset(test, !(cluster %in% c(2, 3, 6, 7, 8, 13, 15)))

temp <- aggregate(temp + 2 ~ cluster, data = train, mean)
train2 <- join(train, temp, by = "cluster", type = "left")

train_sub <- subset(train2, temp + 2 < 27)

full <- lm(frags ~ . + tc_3 * mrvbf, data = train_sub) # "~ ." includes all columns in the data set
null <- lm(frags ~ 1, data = train_sub) # "~ 1" just includes an intercept

add1(null, full, test = "F") # using the AIC test the effect of '


temp <- by(train_sub, list(train_sub$cluster), FUN = function(x) with(x, data.frame(cluster = unique(cluster), r2 = round(cor(frags, predict(test, x))^2, 2), n = length(frags))))
do.call(rbind, temp)
```


## Additional reading

Faraway, J.J., 2002. Practical Regression and Anova using R. CRC Press, New York. [https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf](https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf)

Gareth, J., D. Witten, T. Hastie, and R. Tibshirani, 2014. An Introduction to Statistical Learning: with Applications in R. Springer, New York. [http://www-bcf.usc.edu/~gareth/ISL/](http://www-bcf.usc.edu/~gareth/ISL/)

Hengl, T. 2009. A Practical Guide to Geostatistical Mapping, 2nd Edt. University of Amsterdam, www.lulu.com, 291 p. ISBN 978-90-9024981-0. [http://spatial-analyst.net/book/system/files/Hengl_2009_GEOSTATe2c0w.pdf](http://spatial-analyst.net/book/system/files/Hengl_2009_GEOSTATe2c0w.pdf)

Webster, R. 1997. Regression and functional relations. European Journal of Soil Science, 48, 557-566. [http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2389.1997.tb00222.x/abstract](http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2389.1997.tb00222.x/abstract)
