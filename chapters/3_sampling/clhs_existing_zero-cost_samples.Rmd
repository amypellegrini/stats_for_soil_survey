---
title: "_cLHS_ in R w/ existing (zero-cost) point observations"
author: "Andrew Brown & Dave White [prepared the spatial datasets]"
date: "February 8, 2019"
output: 
  html_document:
    keep_md: no
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First, you should understand the concepts in the __Original Demo by Dave White:__ http://ncss-tech.github.io/stats_for_soil_survey/presentations/examples/clhs.html

# Concepts

## Basics

The basic premise of the _pre-existing data_ extension to _cost-constrained_ Conditioned Latin Hypercube Sampling is that you can "force" a `clhs::clhs()` routine include an arbitrary set  of (user-defined) samples _in your final result_. 

These "forced" data come at "zero-cost" because, well (at least theoretically) you _already have them_. In theory, this _very-low-cost_ partially counteracts the fact that the existing locations are not likely to span the environmental variable space fully. 

## This Demo

This demo uses the demonstration dataset prepared by Dave White. 

To simulate "existing data" a set of 20 random samples is collected from the non-NA extent of the _RasterStack_. These are hypothetical "pre-existing data". 

This small number of random samples are less likely to consistently span the full range of the multivariate data space (the RasterStack), when compared to the exhaustive sampling approach to sampling.

The _RasterStack_ is regularly sampled ("exhaustively"; n=10,000) and these regular samples are passed to the cLHS algorithm for optimization to the user defined number of samples.

Both CLHS routines are run using the cost raster `r.cost`, but the second object created `s.with.existing` is set up to retain the 30 random samples in the last "set" of points cLHS algorithm evaluates. That is, the set that it has when the number of iterations set by the user is reached. 

These observations are appended to the begining of the `all.observations` SpatialPointsDataFrame and indexed using a numeric index `1:30`. The ordering and index choice is important. Note the addition of the `include=1:30` to the `clhs()` call that makes `s.with.existing`.

The final result plot shows the location of cost-constrained cLHS, with and without pre-existing samples on top of the cost raster

__WARNING:__ Use this routine your own risk. The addition to the cLHS algorithm is new and may potentially have unintended effects on clhs result. This addition was just made to the latest official release of `clhs` (was published to CRAN mid-October 2018)

# Sample Code

## Load data into RasterStack

Get the data if you need it.

```{r eval=FALSE}
# create a new directory to store the data
dir.create("C:/workspace2/clhs", recursive = TRUE)

# setting the working directory
setwd("C:/workspace2/clhs/")

# download data
download.file(url = "http://github.com/dave-white2/data/raw/master/clhs/clhs_data.zip", destfile = "clhs.zip", method = "auto")

# unzip data
unzip(zipfile = "C:/workspace2/clhs/clhs.zip", overwrite = TRUE)
```

```{r}
library(rgdal)
library(raster)
library(clhs)

setwd("C:/workspace2/clhs/")

# load raster data of same extent and resolution
r.claymin <- raster("claymin.tif")
r.mrrtf <- raster("mrrtf.tif")
r.mrvbf <- raster("mrvbf.tif")
r.ndvi <- raster("ndvi.tif")
r.sagawi <- raster("sagawi.tif")
r.cost <- raster("cost.tif")

r.stack.cost <- stack(r.claymin, r.mrrtf, r.mrvbf, r.ndvi, r.sagawi, r.cost)
names(r.stack.cost) <- c('claymin', 'mrrtf', 'mrvbf', 'ndvi', 'sagawi', 'cost')
r.stack.cost <- readAll(r.stack.cost)

r.extent.poly <- as(extent(r.stack.cost), "SpatialPolygons")

proj4string(r.extent.poly) <- proj4string(r.stack.cost)
```

## Pre-existing data

Make up some hypothetical "existing observation" locations. 

```{r}
# how many existing sample locations to simulate?
n.existing <- 20
```

We will compare two methods.

The first method we will take a small (unbiased) Simple Random sample.

Then we will take a biased sample that is likely more realistic for pre-existing data. To do this we take a large random sample, and _order it based on cost_. Then from that large sample, we take a stratified subsample -- selected from the _cheaper half of the large random sample_.

These hoops we jump through force the pre-existing data to have somewhat _lower cost_ than is typically obtained from a small Simple Random sample. 
Of course, in practice, bias is present. It is not generally intentional. It arises through real operational constraints as well as personal (sometimes well-informed) biases about good places to sample and/or places that need to be sampled. 

cLHS only gets a raster-based view of the variablity in the landscape. It may well be that it ALONE cannot capture all that is "important" about a landscape. 

```{r}
# existing data as truly random
# we set na.rm = TRUE so our "observations" are not in `NA` raster space
obs.existing <- sampleRandom(r.stack.cost, size = n.existing, sp=TRUE, na.rm=TRUE)

# "realistic" existing data have bias 
# we use the cost raster to bias a subsample of a random sample
obs.rnd <- sampleRandom(r.stack.cost, size = n.existing*10, sp=TRUE, na.rm=TRUE)
obs.rnd <- obs.rnd[order(obs.rnd$cost),]
biased.locations.idx <- floor(runif(n.existing, 1, length(obs.rnd$cost) / 2))
obs.existing.cheap <- obs.rnd[biased.locations.idx,]
```

## Subsample the RasterStack

Use regular sampling to lower the number of cells (see `raster::ncell()`) we are plugging into `clhs()` algorithm.

```{r}
# use regular sampling to lower the number of cells 
reg.samples <- sampleRegular(r.stack.cost, size = 10000, sp = TRUE)
```

## Match Coordinate Reference System

Here we transform ("project") our existing data -- to ensure match the CRS of the regularly spaced subsample. We will be combining these two spatial objects for the second part of `clhs()`.

```{r}
obs.existing <- spTransform(obs.existing, CRS(proj4string(reg.samples)))
obs.existing.cheap <- spTransform(obs.existing.cheap, CRS(proj4string(reg.samples)))
```

Combine existing data with regular (sub)sample.

```{r}
all.observations <- rbind(obs.existing, reg.samples)
all.observations.cheap <- rbind(obs.existing.cheap, reg.samples)
```

Remove any records that may have `NA` cost, pending a fix in _clhs_ package.

```{r}
# pending a fix in clhs package (https://github.com/pierreroudier/clhs/issues/3)
# NA cost values must be filtered from the samples
reg.samples <- reg.samples[which(!is.na(reg.samples$cost)), ]
all.observations <- all.observations[which(!is.na(all.observations$cost)), ]
all.observations.cheap <- all.observations.cheap[which(!is.na(all.observations.cheap$cost)), ]
```

## Cost-constrained c-cLHS

Do the normal cost-constrained cLHS sampling.

```{r}
s <- clhs(reg.samples, 
          size = 100,
          iter = 10000,
          cost = 'cost', 
          simple = FALSE, 
          progress = FALSE)
```

Now the cost-constrained cLHS but force-include the first `r nrow(obs.existing)` row-indexes that correspond to our existing data (that we appended to the _beginning_ of `all.observations`)

```{r}
idx.to.include <-1:nrow(obs.existing)

# note we use this numeric vector of indexes later in the proof of concept
idx.to.include

s.with.existing <- clhs(all.observations, 
                        size = 100, 
                        iter = 10000,
                        cost = 'cost', 
                        include = idx.to.include,  
                        simple = FALSE,
                        progress = FALSE)

s.with.cheap <- clhs(all.observations.cheap, 
                        size = 100, 
                        iter = 10000,
                        cost = 'cost', 
                        include = idx.to.include,  
                        simple = FALSE,
                        progress = FALSE)
```

## Diagnostic plots

_cLHS_object_ diagnostic plot for normal c-cLHS. This is a custom plotting function for the result obtained when the `clhs()` `simple` argument is `FALSE`.

```{r}
plot(s, mode = c("obj", "box"))
```

And... a diagnostic plot for c-cLHS _with existing data_.

```{r}
plot(s.with.existing, mode = c("obj", "box"))
```

Finally, for the "cheap" _existing data_...

```{r}
plot(s.with.cheap, mode = c("obj", "box"))
```

## Spatial comparison of Methods

Create numeric vectors containing indices of the samples selected by the two cost-constrained cCLHS runs.

```{r}
samples.idx <- s$index_samples
existing.idx <- s.with.existing$index_samples
cheap.idx <- s.with.cheap$index_samples
```

Make a plot to show where the cLHS samples are located by the three methods (c-CLHS, c-CLHS with Simple Random pre-existing, c-CLHS with "biased" pre-existing).

The orange points are your Simple Random existing data. Blue are the cLHS observations that come _along with_ your random existing data. 

The purple points are your "biased" existing data. Green are the cLHS observations that come _along with_ your random existing data. 

Red points are obtained from the ordinary (no existing data) cost-constrained cLHS. 

There are 100 points in `BLUE+YELLOW`, 100 points in `PURPLE+GREEN` and 100 in `RED`.

Plot on an environmental variable (_SAGA Wetness Index_) and cost raster contours as a backdrop.

```{r}
# check point locations visually (on the sagawi raster)
par(mar = c(2,2,2,2))

plot(r.sagawi)
contour(r.cost, nlevels=10, col='black', add=TRUE)

 # plot the regularly-spaced samples that were selected by normal cLHS
points(reg.samples[samples.idx, ], bg = 'red', pch=21)

# plot the regularly-spaced samples that were selected when set to retain 
# the existing points that were randomly sampled before running clhs
points(all.observations[existing.idx, ], bg = 'blue', pch=21)
points(all.observations.cheap[cheap.idx, ], bg = 'green', pch=21)

# overplot with yellow to show which ones were the 30 that we started with 
points(obs.existing, bg = 'orange', pch=21)
points(obs.existing.cheap, bg = 'purple', pch=21)

legend(x="topleft",
       legend = c("cost-cLHS","cost-cLHS + Random","cost-cLHS + Biased","Random","Biased"),
       pch=19, col=c("red","blue","green","orange","purple"), bg="#AAAAAA")
```

Plot the same point groups for comparison on Cost Raster itself.

```{r}
# check visualy on the cost raster
par(mar = c(2,2,2,2))

# use cost raster as a backdrop
plot(r.cost)

 # plot the regularly-spaced samples that were selected by normal cLHS
points(reg.samples[samples.idx, ], bg = 'red', pch=21)

# plot the regularly-spaced samples that were selected when set to retain 
# the existing points that were randomly sampled before running clhs
points(all.observations[existing.idx, ], bg = 'blue', pch=21)
points(all.observations.cheap[cheap.idx, ], bg = 'green', pch=21)

# overplot with yellow to show which ones were the 30 that we started with 
points(obs.existing, bg = 'orange', pch=21)
points(obs.existing.cheap, bg = 'purple', pch=21)

legend(x="topleft",
       legend = c("cost-cLHS","cost-cLHS w/ random","cost-cLHS w/ biased","Random","Biased"),
       pch=19, col=c("red","blue","green","orange","purple"), bg="#AAAAAA")
```

Overall, it appears that the randomly sampled samples (yellow) occur at much higher cost postitions than the vast majority of the c-cLHS points and the "biased" points (purple).

The densities of red, blue and green points do appear to be _generally_ similar and clustered in the same types of areas with respect to the `sagatwi` _and_ `cost` raster.

# Proof of Concept

## Ordinary cLHS

For a the purposes of full comparison of methods, we will also sample using _ordinary_ cLHS (without cost constraints). We won't look at these points as we did above, but will have them handy. _These samples still have a theoretical "cost"_ -- it is just not considered in the optimization.

The Ordinary cLHS is important because we can see where the optimal samples would be placed _if cost were no issue_. This is the "ideal" representation.

```{r}
s.ordinary <- clhs(reg.samples, 
                   size = 100,
                   iter = 10000,
                   simple = FALSE, 
                   progress = FALSE)
```

## Combining results for comparison

```{r}
s.all <- rbind(data.frame(method = "cost-cLHS", s$sampled_data@data),
              data.frame(method = "cost-cLHS + Random", s.with.existing$sampled_data@data),
              data.frame(method = "cost-cLHS + Biased", s.with.cheap$sampled_data@data),
              data.frame(method = "Ordinary cLHS", s.ordinary$sampled_data@data),
              data.frame(method = "Random Existing", all.observations[idx.to.include, ]@data),
              data.frame(method = "Cheap Existing", all.observations.cheap[idx.to.include, ]@data),
              data.frame(method = "Exhaustive", reg.samples@data))
```

```{r, echo=FALSE, message=FALSE}
library(data.table)

# housekeeping before reshape and dcast
s.all <- as.data.frame(s.all, stringsAsFactors=FALSE)
s.all$id <- rownames(s.all)
rownames(s.all) <- NULL
var.lut <- 1:6
names(var.lut) <- names(s.all)[2:7]

s.all.long <- reshape(s.all,
                      idvar="id",
                      timevar="variable",
                      v.names="value",
                      direction="long",
                      varying=2:7)

# use actual variable names not numeric index
s.all.long$variable <- names(var.lut)[s.all.long$variable]

# calculate medians for each method X variable combinations
d.mu.wide.no.cost <- dcast(s.all.long[s.all.long$variable != "cost",],
                   method ~ variable, 
                   value.var = 'value',
                   fun=median, na.rm=TRUE)

# and sum for cost of all samples in group
d.mu.wide.just.cost <- dcast(s.all.long[s.all.long$variable == "cost",],
                   method ~ variable, 
                   value.var = 'value',
                   fun=sum, na.rm=TRUE)

total.cost <- d.mu.wide.just.cost[,2]

d.mu.wide <- cbind(d.mu.wide.no.cost, total.cost)
```

As you can see below, the median values obtained from the different methods are quite similar. The cost is calculated as the sum of the costs for all the points in the group.

```{r echo=FALSE}
d.mu.wide.nobs <- cbind(d.mu.wide, 
                        data.frame(n.obs = unlist(lapply(split(s.all, f = s.all$method), nrow))))

# "population" summary (every cell in raster stack)
the.numbers <- c(as.numeric(apply(values(r.stack.cost), 2, median, na.rm=T)),
                 ncell(r.stack.cost))
the.numbers[length(the.numbers)-1] <- sum(values(r.stack.cost[['cost']]), na.rm=T)
df <- data.frame("Population", t(the.numbers), stringsAsFactors = F)
names(df) <- names(d.mu.wide.nobs)

# add population info to sample-based summary data frame
d.mu.wide.nobs <- rbind(d.mu.wide.nobs, df)

knitr::kable(d.mu.wide.nobs, 
      digits=c(4,4,4,4,4),
      row.names = FALSE,
      caption="Comparison of Sample Medians Obtained by Different cLHS Sampling Methods")
```

The cost of the Random Existing samples would be relatively high under ordinary cost-constrained cLHS. The cheap existing samples are a bit less costly, but still are expensive considering that set only gets you 20 observations. _Individual_ observations might be quite "good" in terms of amount of variation they explain -- especially _for their cost_. 

The cost the Random existing samples is high -- but it might be that they can explain a great deal of variation on the landscape... but, they occur in areas that are not _favored_ by the cost constraints.

Since they are provided "free of cost" to the `cost-cLHS w/ Random Existing` sample set, there are likely some benefits to having those data in the set. That said, Random Existing cost suggests they basically "cost more per sample" than samples obtained by _any_ of the cost-constrained cLHS methods (by about a factor of 3-5). It is comparable in per-sample "average cost" to Ordinary cLHS -- which of course does not _care_ about cost. 

This "high cost" for a pre-existing dataset might not be a _realistic_. Operational limitations or personal choices would typically have some sort of bias effect on sampling (at least _relative_ to true Simple Random Sampling). 

This is why we include the test group of _biased_ pre-existing locations. This Cheap Existing group shows a lower cost than the Random group. The cost is lower (per sample) than the Ordinary cLHS approach, but the medians also have some bias relative to the "population" median derived from all cells in the raster. This is partially due to the small sample size.

## Visualizing differences between cLHS methods with nMDS

We will use _non-metric multidimensional scaling_ (nMDS) to visualize any _potential_ multivariate differences between the "optimal" samples obtained by the different methods.

```{r echo=FALSE, message=FALSE}
library(cluster)
library(MASS)

# There are too many samples in the exhaustive set to show in the plot.
# TODO: similar e.g. random resampling from population v.s exhaustive?

is.exhaustive <- s.all$method == "Exhaustive"
# s.all.exhaustive <- s.all[is.exhaustive,]
# s.all.noex <- s.all[!is.exhaustive,]
# # Take 100 of them - using runif() to generate random index
# random.idx <- round(runif(n = 100, min = 1, max = nrow(s.all.exhaustive)))
# s.all.2 <- rbind(s.all.noex,  s.all.exhaustive[random.idx,])

# alternately, do not do any subsampling for population or exhaustive set
s.all.2 <- s.all[!is.exhaustive,]

# remove ID column 
# TODO: should the cost raster be _included_ in the dissimilarity calculations or NOT?
#       the nMDS plot looks "odd" if you leave it out
s.all.2 <- s.all.2[, !names(s.all.2) %in% c("id","cost")]

# calculate dissimilarity matrix
d.dist <- daisy(s.all.2, stand=TRUE)

# map distance matrix to 2D space via principal coordinates
d.betadisper <- vegan::betadisper(d.dist, 
                                  group=s.all.2$method, 
                                  bias.adjust = TRUE, 
                                  sqrt.dist = TRUE, 
                                  type='median')

# get the scores
d.scores <- vegan::scores(d.betadisper)

# add contours for fixed pct of data density using KDE
s <- data.frame(x=d.scores$sites[, 1], y=d.scores$sites[, 2], .id=s.all.2$method)
s <- split(s, s$.id)

# plot
par(mar=c(1,1,3,1))
plot(d.scores$sites, type='n', axes=FALSE)
abline(h=0, v=0, lty=2, col='grey')

# http://stackoverflow.com/questions/16225530/contours-of-percentiles-on-level-plot
kdeContours <- function(i, prob, cols, m, ...) {
  
  if(nrow(i) < 2) {
    return(NULL)
  }
  
  this.id <- unique(i$.id)
  this.col <- cols[match(this.id, m)]
  dens <- kde2d(i$x, i$y, n=200); ## estimate the z counts
  
  dx <- diff(dens$x[1:2])
  dy <- diff(dens$y[1:2])
  sz <- sort(dens$z)
  c1 <- cumsum(sz) * dx * dy
  levels <- sapply(prob, function(x) {
    approx(c1, sz, xout = 1 - x)$y
  })
  
  # add contours if possibly
  if(!is.na(levels))
    contour(dens, levels=levels, drawlabels=FALSE, add=TRUE, col=this.col, ...)
  
  # # add bivariate medians
  # points(median(i$x), median(i$y), pch=3, lwd=2, col=this.col)
}

lvls <- levels(factor(s.all.2$method))
#       "cost-cLHS","cost-cLHS (w/ Random)","cost-cLHS (w/ Biased)","Ordinary cLHS","Existing (Random)","Existing (Biased)"
cols <- c("RED", "BLUE", "GREEN", "BLACK", "ORANGE","PURPLE")

#vegan:::plot.betadisper(d.betadisper, ellipse = TRUE, hull = FALSE, col=cols, conf=0.5, segments=FALSE, xlab='', ylab='', main='', sub='', las=1)

res <- lapply(s, kdeContours, prob=c(0.5), cols=cols, m=lvls, lwd=2, lty=3)
res <- lapply(s, kdeContours, prob=c(0.95), cols=cols, m=lvls, lwd=1, lty=3)

# add individual sample points
points(d.scores$sites, cex=0.6, col=cols[as.numeric(d.mu.wide$method)], pch=16)

vegan::ordilabel(d.betadisper, display='centroids', col=cols[as.numeric(d.mu.wide$method)])

title('Ordination of cLHS Samples by various methods with 50 and 95% Density Contours')

box()
```

This _nMDS_ graphic, though apparently complicated, corroborates the intuition we might have about the effects of including existing, non-optimal data. 

We _omitted the cost raster from the ordination routine_, so that the separations we are seeing are due to true variation in the environment interaction with the different algorithms handling of cost.

We wrap each group's nMDS score point cloud with a 50% (median) density contour so we can view its central tendency more easily.

When projected onto a 2D surface (the plot) the outline of the group hypervolumes appear to be similar, as shown by the similarities (proximity) of their 50% density contour. All groups are spanning a wide range on the X-axis, but are offset on the Y axis.

If we consider the Ordinary cLHS to be the (in practice typically unachievable for Soil Survey) "gold standard", then we can consider any offset from that Ordinary cLHS point cloud to be bias due to our method.

Is the Ordinary cLHS picking up on variation in the landscape that the cost-cLHS methods see as "too costly?" 

_That is how I would interpret this graphic._ 

The _cost-constrained_ results are offset substantially on the vertical axis from the _Ordinary_.

The two clouds of _Existing_ (Cheap & Random) data plot near the origin. The "forced" inclusion of the existing data biases the _cost-cLHS_ in the direction of the _Existing_... and the cost-cLHS that use those existing data are _farther still_ from the "optimal" _Ordinary cLHS_! In the case where your existing data are not _Random_ but rather are are _Cheap_ (low-hanging-fruit) the offset from _Ordinary cLHS_ is even greater.

This would suggest, as expected, that additional bias is introduced to cLHS by additional constraints on _Ordinary cLHS_ method. The degree of bias in the existing data you force-include in _cost-constrained cLHS_ will impact the deviation from the ideal. These differences seen in the nMDS may be essentially numerical noise -- given the precision/accuracy of input raster data and the scaling that occurs during the nMDS process.

Future work will try to re-state differences in cLHS in more practical terms. Further, the `clhs()` code will be reviewed to determine if there is a better way to make use of the information in prior data.

__Food for thought:__ Is the decrease in cost -- i.e. making a project feasible v.s. not -- sufficient to warrant increased bias?

_Should the cost raster be included in the ordination?_